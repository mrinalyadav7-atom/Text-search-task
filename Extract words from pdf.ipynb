{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'enter the name of the file here' \n",
    "#open allows you to read the file.\n",
    "pdfFileObj = open(filename,'rb')\n",
    "#The pdfReader variable is a readable object that will be parsed.\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "#Discerning the number of pages will allow us to parse through all the pages.\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "#The while loop will read each page.\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "#This if statement exists to check if the above library returned words. It's done because PyPDF2 cannot read scanned files\n",
    "if text != \"\":\n",
    "   text = text\n",
    "#If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text.\n",
    "else:\n",
    "   text = textract.process(fileurl, method='tesseract', language='eng')\n",
    "#Now we have a text variable that contains all the text derived from our PDF file. Type print(text) to see what it contains. It likely contains a lot of spaces, possibly junk such as '\\n,' etc.\n",
    "#Now, we will clean our text variable and return it as a list of keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The word_tokenize() function will break our text phrases into individual words.\n",
    "tokens = word_tokenize(text)\n",
    "#We'll create a new list that contains punctuation we wish to clean.\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "#We initialize the stopwords variable, which is a list of words like \"The,\" \"I,\" \"and,\" etc. that don't hold much value as keywords.\n",
    "stop_words = stopwords.words('english')\n",
    "#We create a list comprehension that only returns a list of words that are NOT IN stop_words and NOT IN punctuations.\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
